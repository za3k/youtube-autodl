#!/usr/bin/python3
import asyncio, copy, os, re, sqlitedict, subprocess, sys, time, yaml

# TODO: progress bar?
# TODO: progress bar per-channel?
# TODO: Support reading URLs from a plaintext file in the config

# Cache web fetches
class SqliteExpiringCache():
    def __init__(self, cache_path):
        self.d = sqlitedict.SqliteDict(cache_path)
    def now(self):
        return time.time()
    def get(self, key, expiration_period=None):
        r = self.d.get(key)
        if r is not None:
            value, write_time = r
            if expiration_period is None or write_time + expiration_period >= self.now():
                return value
            del self.d[key] # Expire it
        return None
    def put(self, key, value):
        self.d[key] = (value, self.now())
        self.d.commit()

class AutoDownloader():
    def __init__(self, config_path="config.yaml"):
        with open(config_path, "rb") as f:
            self.config = yaml.safe_load(f)
        self.verify_config()
        # Parse config. (Expand paths and resolve relative paths)
        self.directory = os.path.expanduser(self.config["options"]["directory"])
        self.cache_path = os.path.join(self.directory, os.path.expanduser(self.config["options"]["database"]))
        self.refresh_period = self.config["options"]["refresh_period"]

        self.archive_all = self.config["options"]["archive"]["all"]
        self.archive_directory = os.path.join(self.directory, os.path.expanduser(self.config["options"]["archive"]["directory"]))
        self.archive_format = self.config["options"]["archive"]["format"]
        if self.config["options"]["archive"]["download_archive"]:
            self.download_archive = os.path.join(self.directory, os.path.expanduser(self.config["options"]["archive"]["download_archive"]))
        else:
            self.download_archive = None
        self.autodelete_ = self.config["options"]["archive"]["autodelete"]
        self.deduplicate = self.config["options"]["archive"]["deduplicate"]

        self.binary = self.config["options"]["youtube-dl"]["binary"]
        self.video_arguments = self.config["options"]["youtube-dl"]["always_arguments"] + self.config["options"]["youtube-dl"]["video_arguments"]
        self.playlist_arguments = self.config["options"]["youtube-dl"]["always_arguments"] + self.config["options"]["youtube-dl"]["playlist_arguments"]
        
        # Non-config initialization
        self.cache = SqliteExpiringCache(self.cache_path)
        self.download_queue = asyncio.Queue()
        self.downloaded_videos = self.cache.get("downloaded_videos") or set()
        if self.download_archive:
            try:
                with open(self.download_archive) as f:
                    self.downloaded_video_ids = {line.split()[1] for line in f}
            except FileNotFoundError:
                self.downloaded_video_ids = set()
        else:
            self.downloaded_video_ids = set()


    def verify_config(self):
        pass # Figure out how to do this declaratively. Until then skip it.

    async def run_once(self):
        os.makedirs(self.directory, exist_ok=True)
        os.makedirs(self.archive_directory, exist_ok=True)
        os.chdir(self.directory)

        await self.download()
        if self.autodelete_:
            self.autodelete()
    def autodelete(self):
        assert self.deduplicate == "hardlink", "options.archive.autodelete=true requires options.archive.deduplicate=hardlink"
        for file in os.scandir(self.archive_directory):
            if file.name == os.path.basename(self.download_archive): # Don't delete ARCHIVE.txt
                continue
            if file.stat(follow_symlinks=False).st_nlink == 1:
                print("Auto-deleting: ", file.path)
                os.unlink(file.path)

    async def download(self):
        # Grab list of videos, QUEUE them but don't download yet
        await asyncio.gather(*[self._download_watchlist_item(**watchlist_item) for watchlist_item in self.config["watchlist"]])
        # OK, download videos.
        print("Videos queued for download: ", self.download_queue.qsize())
        while not self.download_queue.empty():
            task = await self.download_queue.get()
            await task
    async def _download_watchlist_item(self, name=None, channel=None, playlist=None, search=None, video=None, save=None, note=None, save_playlists=None, yt_dl_args=[], **kwargs):
        assert len(kwargs) == 0

        urls = {channel, playlist, search, video} # Exactly  one should be set
        assert None in urls and len(urls) == 2

        kwargs = {
            "save": set(save),
            "yt_dl_args": yt_dl_args,
            "name": name or url,
            "note": note,
        }
        if playlist:
            await self.download_playlist(playlist, **kwargs)
        elif channel:
            await self.download_channel(channel, save_playlists=save_playlists, **kwargs)
        elif search:
            await self.download_search(search, **kwargs)
        elif video:
            await self.queue_download_video(video, **kwargs)

    async def download_playlist(self, playlist_url, **kwargs):
        await self.download_videos("playlist", playlist_url, **kwargs)

    async def download_channel(self, channel_url, save_playlists=None, **kwargs):
        await self.download_videos("channel", channel_url, **kwargs)

        # Also download all the playlists for the channel
        if save_playlists is not None:
            tasks = []
            for playlist_url in await self.get_playlists_for_channel(channel_url, **kwargs):
                #print("Discovered {} playlist: {}".format(kwargs.get("name"), playlist_url))
                kwargs2 = copy.copy(kwargs)
                kwargs2["save"] = save_playlists
                kwargs2["name"] = "{} ({})".format(kwargs.get("name"), playlist_url)
                tasks.append(self._get_urls_for("playlist", playlist_url, **kwargs2))
            await asyncio.gather(*tasks)
    async def get_playlists_for_channel(self, channel_url, **kwargs):
        playlists_url = channel_url + "/playlists?view=1" # Just playlists, not
        item_urls = await self._get_urls_for("channel_playlist", playlists_url, ignore_failure=True, **kwargs) # A transient failure
        if item_urls is None or len(item_urls) == 0:
            print("Failed to download playlists for channel: ", name)
            return []
        tasks = []
        playlists = []
        for item_url in item_urls:
            if "shelf_id=" in item_url and "/playlists" in item_url:
                # Might be a shelf:    https://www.youtube.com/c/ImpAndSkizz/playlists?view=1&sort=dd&shelf_id=0
                #print("Discovered shelf: ", item_url)
                tasks.append(self.get_playlists_for_shelf(item_url, **kwargs))
            elif "list=" in item_url:
                # Might be a playlist: https://www.youtube.com/playlist?list=PLt673TcEuoVz1o5qyUV4MpYtW8jt-5t1p
                playlists.append(item_url)
        results = await asyncio.gather(*tasks)
        for playlist_urls in results:
            playlists.extend(playlist_urls)
        return playlists
    async def get_playlists_for_shelf(self, shelf_url, **kwargs):
        return await self._get_urls_for("shelf", shelf_url, **kwargs)

    async def download_search(self, search_term, **kwargs):
        await self.download_videos("search", "ytsearch:" + search_term, **kwargs)

    async def download_videos(self, url_type, url, **kwargs):
        video_urls = await self._get_urls_for(url_type, url, **kwargs)
        if video_urls is not None:
            #print("list of videos from {} {}: {}".format(url_type, kwargs.get("name", url), [len(video_ids)]+video_ids[:10]))
            await asyncio.gather(*(self.queue_download_video(video_url, **kwargs) for video_url in video_urls))

    async def _get_urls_for(self, url_type, url, ignore_failure=False, **kwargs):
        # Caching (and no retries) layer over _fetch_urls_for
        assert url_type is not None
        result = self.cache.get("_get_urls_for "+url, self.refresh_period) # Fetch from cache
        if result is not None:
            return result
        result = await self._fetch_urls_for(url_type, url, ignore_failure=ignore_failure, **kwargs)
        if result is None: # Fail to calculate result? Return immediately with no retries
            return result
        self.cache.put("_get_urls_for "+url, result) # Store result if calculated
        return result
    async def _fetch_urls_for(self, url_type, url, ignore_failure=False, **kwargs):
        assert url_type is not None
        print("Updating {}: {}".format(url_type,  kwargs.get("name", url)))
        if url_type in ["channel", "channel_playlist", "shelf", "playlist"]:
            # --flat-playlist is MUCH faster than --skip-download
            command = [self.binary, "--flat-playlist", "--print", "url", "-s", url] + self.playlist_arguments
        elif url_type in ["search"]:
            # -i skips bad videos, including age-restricted ones. add cookies to get around age-restriction.
            command = [self.binary, "--skip-download", "-i", "--get-filename", "-s", "-o", "%(url)s", url] + self.playlist_arguments
        else:
            assert False, "Wrong type {} for: {}".format(url_type, url)
        exit_code, stdout, stderr = await self.run(command)
        if exit_code == 0 or ignore_failure:
            print("Updated {}: {}".format(url_type, kwargs.get("name", url)))
            lines = [x.strip() for x in stdout.decode('ascii').split("\n")]
            ids = [line.strip() for line in lines if line != "NA" and line != ""]
            if len(ids) > 0:
                return ids
        else:
            print("fetch_list failed for {}: {}".format(url_type, kwargs.get("name", url)))
            print(" ".join(command))
            print(stderr.decode('utf8'))
    async def run(self, command):
        process = await asyncio.create_subprocess_exec(
            *command, 
            stdout=asyncio.subprocess.PIPE, 
            stderr=asyncio.subprocess.PIPE,
        )
        (output, err) = await process.communicate()
        exit_code = process.returncode
        return exit_code, output, err

    async def queue_download_video(self, video_url, **kwargs):
        if video_url in self.downloaded_videos:
            master_filepath = self.cache.get("video_path " + video_url)
            if master_filepath is not None and os.path.exists(master_filepath):
                return
        await self.download_queue.put(self.download_video(video_url, **kwargs))

    async def download_video(self, video_url, **kwargs):
        formats = [] # List of formats. First is the "primary" version for symlink purposes.
        for save_format in kwargs.get("save", []):
            if save_format not in formats:
                formats.append(save_format)
        if self.archive_all:
            if self.archive_format in formats:
                formats.remove(self.archive_format)
            file_format, additional_formats = self.archive_format, formats
        else:
            file_format, additional_formats = formats[0], formats[1:]

        command = [self.binary]
        command += ["--no-progress"] 
        if self.download_archive:
            command += ["--download-archive", self.download_archive]
        command += ["-o", file_format]
        for additional_format in additional_formats:
            # We can't combined %(template)S and %(template)q
            # And indeed, we might not want full sanitization.
            # Instead, we write {{%(title)q}}s
            # Then we pass {{weird title/whatever}}s to ourselves, which strips out characters such as / we don't want in the filename
            magic_format = re.sub("%\(([a-z]+)\)([sSq])", r"{{%(\1)q}}\2", additional_format)
            # OK but we also need to fix all the stuff not inside a string..? For now just special-case some stuff.
            magic_format = magic_format.replace(" ", "\\ ")
            command += ["--exec", '"{}" --deduplicate {} %(filepath)q {}'.format(sys.argv[0], self.deduplicate, magic_format)]
        command += self.video_arguments
        command += ["-q", "--exec", "echo"] # record the saved master file
        command += [video_url]

        print("Downloading for {}: {}".format(kwargs.get("name"), video_url))
        exit_code, stdout, stderr = await self.run(command)
        filepath = stdout.decode("utf8").strip("\n")
        if exit_code != 0:
            print(stdout.decode('utf8'))
            print(stderr.decode('utf8'))
            return

        self.downloaded_videos.add(video_url)
        self.cache.put("downloaded_videos", self.downloaded_videos)
        # This fails if ARCHIVE.txt has the ID before youtube-autodl knows about it
        self.cache.put("video_path " + video_url, filepath) # remember where it was downloaded to, and skip if it's present next time

if __name__ == "__main__":
    if len(sys.argv) == 1:
        yadl = AutoDownloader("/home/zachary/youtube-autodl/config.yaml")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(yadl.run_once())
    elif sys.argv[1:2] == ["--deduplicate"] and len(sys.argv) == 5:
        deduplicate_method, master_filepath, magic_string = sys.argv[2:]

        def replace_magic(magic_bit):
            middle, formatter = magic_bit.group(1), magic_bit.group(2)
            # TODO: if formatter is S, sanitize. Also, deal with #S.
            return middle.replace("/", "_")
        target_filepath = re.sub("{{(.*?)}}([sSq])", replace_magic, magic_string)

        target_dir = os.path.dirname(target_filepath)
        os.makedirs(target_dir, exist_ok=True)

        copier = { # TODO: Should this delete stuff (-f)?
            "none": ["cp", "-f"],
            "symlink": ["ln", "-sf"],
            "hardlink": ["ln", "-f"],
        }[deduplicate_method]
        command = copier + [master_filepath, target_filepath]
        subprocess.run(command)
    else:
        assert False, "Run with no arguments, please. {}".format(sys.argv)
        print(sys.argv)
        sys.exit(1)
